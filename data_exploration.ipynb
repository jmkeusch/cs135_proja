{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8') # pretty matplotlib plots\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme('notebook', style='whitegrid', font_scale=1.25)\n",
    "\n",
    "# autoload changes in other files, so you don't have to restart the Jupyter kernel each time you make a change to the imported code.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "import sklearn.linear_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experimentloop import load_data, pipeline, grid_search, output_grid_result, C_GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_NC, y_N = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline\n",
      "best score:  0.5\n",
      "best params:  {'classify': DummyClassifier(constant=0, strategy='constant')}\n",
      "vocab size:  4510\n",
      "DummyClassifier(constant=0, strategy='constant')\n"
     ]
    }
   ],
   "source": [
    "output_grid_result('baseline', grid_search(x_NC, y_N, {\n",
    "    \"classify\": [DummyClassifier(strategy=\"constant\", constant=0)]\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_default\n",
      "best score:  0.872\n",
      "best params:  {'classify__C': 3.162, 'featurize': CountVectorizer()}\n",
      "vocab size:  4510\n",
      "LogisticRegression(C=3.1622776601683795, max_iter=400)\n"
     ]
    }
   ],
   "source": [
    "output_grid_result('count_default', grid_search(x_NC, y_N, {\n",
    "    \"featurize\": [CountVectorizer()],\n",
    "    \"classify__C\": C_GRID,\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_binary\n",
      "best score:  0.872\n",
      "best params:  {'classify__C': 3.162, 'featurize': CountVectorizer(binary=True)}\n",
      "vocab size:  4510\n",
      "LogisticRegression(C=3.1622776601683795, max_iter=400)\n"
     ]
    }
   ],
   "source": [
    "output_grid_result('count_binary', grid_search(x_NC, y_N, {\n",
    "    \"featurize\": [CountVectorizer(binary=True)],\n",
    "    \"classify__C\": C_GRID,\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_3chars\n",
      "best score:  0.887\n",
      "best params:  {'classify': LogisticRegression(C=5.623413251903491, max_iter=600), 'classify__C': 5.623, 'featurize': TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')}\n",
      "vocab size:  4412\n",
      "LogisticRegression(C=5.623413251903491, max_iter=600)\n"
     ]
    }
   ],
   "source": [
    "output_grid_result('tfidf_3chars', grid_search(x_NC, y_N, {\n",
    "    \"featurize\": [TfidfVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w\\w\\w+\\b\")],\n",
    "    'classify': [LogisticRegression(solver='lbfgs', max_iter=600)],\n",
    "    \"classify__C\": np.logspace(-9, 6, 36),\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_3chars\n",
      "best score:  0.887\n",
      "best params:  {'classify': LogisticRegression(C=5.623413251903491, max_iter=600), 'classify__C': 5.623, 'featurize': TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')}\n",
      "vocab size:  4412\n",
      "LogisticRegression(C=5.623413251903491, max_iter=600)\n"
     ]
    }
   ],
   "source": [
    "output_grid_result('tfidf_3chars', grid_search(x_NC, y_N, {\n",
    "    \"featurize\": [TfidfVectorizer(lowercase=True, token_pattern=r\"(?u)\\b\\w\\w\\w+\\b\")],\n",
    "    'classify': [LogisticRegression(solver='lbfgs', max_iter=600)],\n",
    "    \"classify__C\": np.logspace(0, 1, 21),\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '00'),\n",
       " (30, '10'),\n",
       " (3, '100'),\n",
       " (2, '11'),\n",
       " (4, '12'),\n",
       " (3, '13'),\n",
       " (2, '15'),\n",
       " (1, '15g'),\n",
       " (1, '15pm'),\n",
       " (2, '17'),\n",
       " (1, '18'),\n",
       " (1, '18th'),\n",
       " (1, '1928'),\n",
       " (1, '1948'),\n",
       " (1, '1971'),\n",
       " (1, '1973'),\n",
       " (1, '1979'),\n",
       " (1, '1980'),\n",
       " (1, '1986'),\n",
       " (1, '1995'),\n",
       " (1, '1998'),\n",
       " (6, '20'),\n",
       " (1, '2000'),\n",
       " (1, '2007'),\n",
       " (1, '20th'),\n",
       " (1, '2160'),\n",
       " (1, '24'),\n",
       " (2, '25'),\n",
       " (4, '30'),\n",
       " (1, '30s'),\n",
       " (1, '325'),\n",
       " (2, '35'),\n",
       " (1, '350'),\n",
       " (1, '375'),\n",
       " (1, '3o'),\n",
       " (5, '40'),\n",
       " (1, '40min'),\n",
       " (1, '42'),\n",
       " (1, '44'),\n",
       " (2, '45'),\n",
       " (1, '4s'),\n",
       " (1, '4ths'),\n",
       " (4, '50'),\n",
       " (1, '5020'),\n",
       " (3, '510'),\n",
       " (1, '5320'),\n",
       " (1, '54'),\n",
       " (1, '5lb'),\n",
       " (1, '680'),\n",
       " (2, '70'),\n",
       " (1, '70000'),\n",
       " (1, '700w'),\n",
       " (2, '80'),\n",
       " (1, '80s'),\n",
       " (1, '8125'),\n",
       " (1, '85'),\n",
       " (1, '8525'),\n",
       " (1, '8530'),\n",
       " (1, '8pm'),\n",
       " (6, '90'),\n",
       " (1, '95'),\n",
       " (1, '99')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = {}\n",
    "cv = CountVectorizer().fit(x_NC.loc[:, 'text'])\n",
    "all_counts = cv.transform(x_NC.loc[:, 'text']).sum(axis=0).A1\n",
    "ix_to_word = {v: k for k, v in cv.vocabulary_.items()}\n",
    "count_word_pairs = [(c, ix_to_word[i]) for i, c in enumerate(all_counts)]\n",
    "[(c, w) for c, w in count_word_pairs if re.match(r'[0-9]', w)]\n",
    "#' '.join([w for c, w in sorted([(c, w) for c, w in count_word_pairs if c <= 1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_max_90pct_docs\n",
      "best score:  0.873\n",
      "best params:  {'classify': LogisticRegression(max_iter=400, solver='liblinear'), 'classify__C': 1.0, 'featurize': CountVectorizer(max_df=0.10612244897959185), 'featurize__max_df': 0.106}\n",
      "vocab size:  4501\n",
      "LogisticRegression(max_iter=400, solver='liblinear')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theo/opt/miniconda3/envs/cs135_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "155 fits failed out of a total of 7750.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "155 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/theo/opt/miniconda3/envs/cs135_env/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/theo/opt/miniconda3/envs/cs135_env/lib/python3.10/site-packages/sklearn/pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/theo/opt/miniconda3/envs/cs135_env/lib/python3.10/site-packages/sklearn/pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/Users/theo/opt/miniconda3/envs/cs135_env/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/theo/opt/miniconda3/envs/cs135_env/lib/python3.10/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/Users/theo/opt/miniconda3/envs/cs135_env/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1398, in fit_transform\n",
      "    raise ValueError(\"max_df corresponds to < documents than min_df\")\n",
      "ValueError: max_df corresponds to < documents than min_df\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/theo/opt/miniconda3/envs/cs135_env/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.70924306 0.77578472 ... 0.83308333 0.83321181 0.83314931]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "output_grid_result('count_max_90pct_docs', grid_search(x_NC, y_N, {\n",
    "    \"featurize\": [CountVectorizer()],\n",
    "    \"featurize__max_df\": np.linspace(0.0, 0.2, 50),\n",
    "    'classify': [LogisticRegression(solver='liblinear', max_iter=400)],\n",
    "    \"classify__C\": C_GRID,\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       websiteamazon Oh and I forgot to also mention ...\n",
       "1              websiteamazon THAT one didn't work either.\n",
       "2                        websiteamazon Waste of 13 bucks.\n",
       "3       websiteamazon Product is useless, since it doe...\n",
       "4       websiteamazon None of the three sizes they sen...\n",
       "                              ...                        \n",
       "2395    websiteyelp The sweet potato fries were very n...\n",
       "2396    websiteyelp I could eat their bruschetta all d...\n",
       "2397                     websiteyelp Ambience is perfect.\n",
       "2398    websiteyelp We ordered the duck rare and it wa...\n",
       "2399    websiteyelp Service was nice and the company w...\n",
       "Length: 2400, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'website' + x_NC['website_name'] + ' ' + x_NC['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_with_website\n",
      "best score:  0.872\n",
      "best params:  {'classify__C': 3.162, 'extract_text': FunctionTransformer(func=<function <lambda> at 0x1799815a0>), 'featurize': CountVectorizer()}\n",
      "vocab size:  4513\n",
      "LogisticRegression(C=3.1622776601683795, max_iter=400)\n"
     ]
    }
   ],
   "source": [
    "output_grid_result('count_with_website', grid_search(x_NC, y_N, {\n",
    "    #\"extract_text\": [FunctionTransformer(lambda x_NC: x_NC.agg(' '.join, axis=1))],\n",
    "    \"extract_text\": [FunctionTransformer(lambda x_NC: 'website' + x_NC['website_name'] + ' ' + x_NC['text'])],\n",
    "    \"featurize\": [CountVectorizer()],\n",
    "    \"classify__C\": C_GRID,\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theo/opt/miniconda3/envs/cs135_env/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_top_n_words\n",
      "best score:  0.872\n",
      "best params:  {'classify__C': 3.162, 'featurize': CountVectorizer(max_features=3952), 'featurize__max_features': 3952}\n",
      "vocab size:  3952\n",
      "LogisticRegression(C=3.1622776601683795, max_iter=400)\n"
     ]
    }
   ],
   "source": [
    "output_grid_result('count_top_n_words', grid_search(x_NC, y_N, {\n",
    "    \"featurize\": [CountVectorizer()],\n",
    "    \"featurize__max_features\": np.linspace(2000, 4510, 10, dtype='int'),\n",
    "    \"classify__C\": C_GRID,\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_default\n",
      "best score:  0.885\n",
      "best params:  {'classify__C': 10.0, 'featurize': TfidfVectorizer()}\n",
      "vocab size:  4510\n",
      "LogisticRegression(C=10.0, max_iter=400)\n"
     ]
    }
   ],
   "source": [
    "output_grid_result('tfidf_default', grid_search(x_NC, y_N, {\n",
    "    \"featurize\": [TfidfVectorizer()],\n",
    "    \"classify__C\": C_GRID,\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    import re\n",
    "    import string\n",
    "    text = text.lower()\n",
    "    text = re.sub('[.*?]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('[\\d\\n]', ' ', text)\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs135_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
